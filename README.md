# awesome4me (Paper and Code)

### Image to Image Translation

#### 2023
- <a href='https://arxiv.org/abs/2303.08622'>Zero-Shot Contrastive loss for Text guided diffusion image style transfer</a>

#### 2022
- <a href='https://arxiv.org/pdf/2301.04685v1.pdf'>SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation</a>

### Image Synthesis
#### 2023
- <a href='https://arxiv.org/abs/2302.10167'>Cross-domain Compositing with Pretrained Diffusion Models</a>
### Robust Model (Augmentation, Domain shift, etc..)
#### 2023
- <a href='https://arxiv.org/pdf/2304.08466.pdf'>Synthetic Data from Diffusion Models Improves ImageNet Classification</a>

#### 2022
- <a href='https://arxiv.org/abs/2209.07522'>Test-Time Training with Masked Autoencoders</a>
#### 2019
- <a href='https://arxiv.org/abs/1907.07484'>Benchmarking Robustness in Object Detection: Autonomous Driving when winter is coming</a>

### CLIP code

- <a href='https://github.com/kuai-lab/sound-guided-semantic-image-manipulation/blob/main/soundclip/train.py'>CLIP Training</a> (not official but easy reference code)
- <a href='https://github.com/mlfoundations/open_clip'>Open CLIP</a> (semi official)
