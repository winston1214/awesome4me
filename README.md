# awesome4me (Paper and Code)

### Image to Image Translation

#### 2023
- <a href='https://arxiv.org/abs/2303.08622'>Zero-Shot Contrastive loss for Text guided diffusion image style transfer</a>
[<a href='https://github.com/ZeConloss/ZeCon/tree/main/guided_diffusion'>code</a>]

#### 2022
- <a href='https://arxiv.org/pdf/2301.04685v1.pdf'>SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation</a>
- <a href='https://arxiv.org/pdf/2210.05559.pdf'>Unifying Diffusion Modelâ€™ Latent Space, With Aapplications to Cyclediffusion and guidance

### Image Synthesis
#### 2023
- <a href='https://arxiv.org/pdf/2304.09728'>Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate</a>
- <a href='https://arxiv.org/abs/2302.10167'>Cross-domain Compositing with Pretrained Diffusion Models</a>
#### 2022
- <a href='https://arxiv.org/abs/2111.14818'>Blended Diffusion for Text-driven Editing of Natural Images</a>
#### 2021
- <a href='https://arxiv.org/pdf/2108.03647'>AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer</a>

### Robust Model (Augmentation, Domain shift, etc..)
#### 2023
- <a href='https://arxiv.org/pdf/2304.08466.pdf'>Synthetic Data from Diffusion Models Improves ImageNet Classification</a>

#### 2022
- <a href='https://arxiv.org/abs/2209.07522'>Test-Time Training with Masked Autoencoders</a>
#### 2019
- <a href='https://arxiv.org/abs/1907.07484'>Benchmarking Robustness in Object Detection: Autonomous Driving when winter is coming</a>

### Sign Language

#### 2022
- <a href='https://openaccess.thecvf.com/content/CVPR2022/papers/Yin_MLSLT_Towards_Multilingual_Sign_Language_Translation_CVPR_2022_paper.pdf'>MLSLT: Towards Multilingual Sign Language Translation
</a>

### CLIP code

- <a href='https://github.com/openai/CLIP/tree/a9b1bf5920416aaeaec965c25dd9e8f98c864f16/clip'>CLIP</a> (Official)
- <a href='https://github.com/kuai-lab/sound-guided-semantic-image-manipulation/blob/main/soundclip/train.py'>CLIP Training</a> (not official but easy reference code)
- <a href='https://github.com/mlfoundations/open_clip'>Open CLIP</a> (semi official)
